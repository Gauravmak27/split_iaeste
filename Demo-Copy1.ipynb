{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pprint\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import pickle\n",
    "import math\n",
    "import cv2\n",
    "import copy\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D, Dropout\n",
    "from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, TimeDistributed\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.objectives import categorical_crossentropy\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.utils import generic_utils\n",
    "from keras.engine import Layer, InputSpec\n",
    "from keras import initializers, regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Config Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # Print the process or not\n",
    "        self.verbose = True\n",
    "\n",
    "        # Name of base network\n",
    "        self.network = 'vgg'\n",
    "\n",
    "        # Setting for data augmentation\n",
    "        self.use_horizontal_flips = False\n",
    "        self.use_vertical_flips = False\n",
    "        self.rot_90 = False\n",
    "\n",
    "        # Anchor box scales\n",
    "        \n",
    "        # Note that if im_size is smaller, anchor_box_scales should be scaled\n",
    "        \n",
    "        self.anchor_box_scales = [64, 128, 256] \n",
    "\n",
    "        # Anchor box ratios\n",
    "        self.anchor_box_ratios = [[1, 1], [1./math.sqrt(2), 2./math.sqrt(2)], [2./math.sqrt(2), 1./math.sqrt(2)]]\n",
    "\n",
    "        # Size to resize the smallest side of the image\n",
    "        # Original setting in paper is 600. Set to 300 in here to save training time\n",
    "        self.im_size = 300\n",
    "\n",
    "        # image channel-wise mean to subtract\n",
    "        self.img_channel_mean = [103.939, 116.779, 123.68]\n",
    "        self.img_scaling_factor = 1.0\n",
    "        \n",
    "        # number of ROIs at once\n",
    "        self.num_rois = 4\n",
    "\n",
    "        # stride at the RPN (this depends on the network configuration)\n",
    "        self.rpn_stride = 16\n",
    "\n",
    "        self.balanced_classes = False\n",
    "\n",
    "        # scaling the stdev\n",
    "        self.std_scaling = 4.0\n",
    "        self.classifier_regr_std = [8.0, 8.0, 4.0, 4.0]\n",
    "        # overlaps for RPN\n",
    "        self.rpn_min_overlap = 0.3\n",
    "        self.rpn_max_overlap = 0.7\n",
    "        \n",
    "        # overlaps for classifier ROIs\n",
    "        self.classifier_min_overlap = 0.1\n",
    "        self.classifier_max_overlap = 0.5\n",
    "\n",
    "        # placeholder for the class mapping, automatically generated by the parser\n",
    "        self.class_mapping = None\n",
    "\n",
    "        self.model_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parser data annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(input_path = 'New attempt\\Images'):\n",
    "    \n",
    "    #Parser the data from annotation file\n",
    "\n",
    "    found_bg = False\n",
    "    \n",
    "    all_imgs = {}\n",
    "\n",
    "    classes_count = {}\n",
    "\n",
    "    class_mapping = {}\n",
    "\n",
    "    visualise = True\n",
    "\n",
    "    i = 1\n",
    "    \n",
    "    \n",
    "    with open(input_path,'r') as f:\n",
    "\n",
    "        print('Parsing annotation files')\n",
    "        \n",
    "        for line in f:\n",
    "\n",
    "            # Print process\n",
    "            sys.stdout.write('\\r'+'idx=' + str(i))\n",
    "            i += 1\n",
    "\n",
    "            line_split = line.strip().split(',')\n",
    "\n",
    "            # Make sure the info saved in annotation file matching the format (path_filename, x1, y1, x2, y2, class_name)\n",
    "            # Note:\n",
    "            #\tOne path_filename might has several classes (class_name)\n",
    "            #\tx1, y1, x2, y2 are the pixel value of the origial image, not the ratio value\n",
    "            #\t(x1, y1) top left coordinates; (x2, y2) bottom right coordinates\n",
    "            #   x1,y1--------------------\n",
    "            #\t|\t\t\t\t\t\t|\n",
    "            #\t|\t\t\t\t\t\t|\n",
    "            #\t|\t\t\t\t\t\t|\n",
    "            #\t|\t\t\t\t\t\t|\n",
    "            #\t----------------------x2,y2\n",
    "\n",
    "            (filename,x1,y1,x2,y2,class_name) = line_split\n",
    "            \n",
    "            if class_name not in classes_count:\n",
    "                classes_count[class_name] = 1\n",
    "            else:\n",
    "                classes_count[class_name] += 1\n",
    "\n",
    "            if class_name not in class_mapping:\n",
    "                if class_name == 'bg' and found_bg == False:\n",
    "                    print('Found class name with special name bg. Will be treated as a background region (this is usually for hard negative mining).')\n",
    "                    found_bg = True\n",
    "                class_mapping[class_name] = len(class_mapping)\n",
    "\n",
    "            if filename not in all_imgs:\n",
    "                all_imgs[filename] = {}\n",
    "            \n",
    "                img = cv2.imread(filename)\n",
    "                (rows,cols) = img.shape[:2]\n",
    "                all_imgs[filename]['filepath'] = filename\n",
    "                all_imgs[filename]['width'] = cols\n",
    "                all_imgs[filename]['height'] = rows\n",
    "                all_imgs[filename]['bboxes'] = []\n",
    "                # if np.random.randint(0,6) > 0:\n",
    "                # \tall_imgs[filename]['imageset'] = 'trainval'\n",
    "                # else:\n",
    "                # \tall_imgs[filename]['imageset'] = 'test'\n",
    "\n",
    "            all_imgs[filename]['bboxes'].append({'class': class_name, 'x1': int(x1), 'x2': int(x2), 'y1': int(y1), 'y2': int(y2)})\n",
    "\n",
    "\n",
    "    all_data = []\n",
    "    for key in all_imgs:\n",
    "            all_data.append(all_imgs[key])\n",
    "        \n",
    "        # make sure the bg class is last in the list\n",
    "    if found_bg:\n",
    "         if class_mapping['bg'] != len(class_mapping) - 1:\n",
    "                key_to_switch = [key for key in class_mapping.keys() if class_mapping[key] == len(class_mapping)-1][0]\n",
    "                val_to_switch = class_mapping['bg']\n",
    "                class_mapping['bg'] = len(class_mapping) - 1\n",
    "                class_mapping[key_to_switch] = val_to_switch\n",
    "        \n",
    "    return all_data, classes_count, class_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROI poolinng convolutional layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoiPoolingConv(Layer):\n",
    "\n",
    "    \n",
    "    def __init__(self, pool_size, num_rois, **kwargs):\n",
    "\n",
    "        self.dim_ordering = K.image_dim_ordering()\n",
    "        self.pool_size = pool_size\n",
    "        self.num_rois = num_rois\n",
    "\n",
    "        super(RoiPoolingConv, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.nb_channels = input_shape[0][3]   \n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return None, self.num_rois, self.pool_size, self.pool_size, self.nb_channels\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "\n",
    "        assert(len(x) == 2)\n",
    "\n",
    "        # x[0] is image with shape (rows, cols, channels)\n",
    "        img = x[0]\n",
    "\n",
    "        # x[1] is roi with shape (num_rois,4) with ordering (x,y,w,h)\n",
    "        rois = x[1]\n",
    "\n",
    "        input_shape = K.shape(img)\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for roi_idx in range(self.num_rois):\n",
    "\n",
    "            x = rois[0, roi_idx, 0]\n",
    "            y = rois[0, roi_idx, 1]\n",
    "            w = rois[0, roi_idx, 2]\n",
    "            h = rois[0, roi_idx, 3]\n",
    "\n",
    "            x = K.cast(x, 'int32')\n",
    "            y = K.cast(y, 'int32')\n",
    "            w = K.cast(w, 'int32')\n",
    "            h = K.cast(h, 'int32')\n",
    "\n",
    "            # Resized roi of the image to pooling size (7x7)\n",
    "            rs = tf.image.resize_images(img[:, y:y+h, x:x+w, :], (self.pool_size, self.pool_size))\n",
    "            outputs.append(rs)\n",
    "                \n",
    "\n",
    "        final_output = K.concatenate(outputs, axis=0)\n",
    "\n",
    "        # Reshape to (1, num_rois, pool_size, pool_size, nb_channels)\n",
    "        # Might be (1, 4, 7, 7, 3)\n",
    "        final_output = K.reshape(final_output, (1, self.num_rois, self.pool_size, self.pool_size, self.nb_channels))\n",
    "\n",
    "        # permute_dimensions is similar to transpose\n",
    "        final_output = K.permute_dimensions(final_output, (0, 1, 2, 3, 4))\n",
    "\n",
    "        return final_output\n",
    "    \n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {'pool_size': self.pool_size,\n",
    "                  'num_rois': self.num_rois}\n",
    "        base_config = super(RoiPoolingConv, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_output_length(width, height):\n",
    "    def get_output_length(input_length):\n",
    "        return input_length//16\n",
    "\n",
    "    return get_output_length(width), get_output_length(height)    \n",
    "\n",
    "def nn_base(input_tensor=None, trainable=False):\n",
    "\n",
    "\n",
    "    input_shape = (None, None, 3)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "    bn_axis = 3\n",
    "\n",
    "    # Block 1\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "    # x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RPN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rpn_layer(base_layers, num_anchors):\n",
    "    \n",
    "    x = Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv1')(base_layers)\n",
    "\n",
    "    x_class = Conv2D(num_anchors, (1, 1), activation='sigmoid', kernel_initializer='uniform', name='rpn_out_class')(x)\n",
    "    x_regr = Conv2D(num_anchors * 4, (1, 1), activation='linear', kernel_initializer='zero', name='rpn_out_regress')(x)\n",
    "\n",
    "    return [x_class, x_regr, base_layers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classifier_layer(base_layers, input_rois, num_rois, nb_classes = 2): \n",
    "\n",
    "    input_shape = (num_rois,7,7,512)\n",
    "\n",
    "    pooling_regions = 7\n",
    "\n",
    "    # out_roi_pool.shape = (1, num_rois, channels, pool_size, pool_size)\n",
    "    # num_rois (4) 7x7 roi pooling\n",
    "    out_roi_pool = RoiPoolingConv(pooling_regions, num_rois)([base_layers, input_rois])\n",
    "\n",
    "    # Flatten the convlutional layer and connected to 2 FC and 2 dropout\n",
    "    out = TimeDistributed(Flatten(name='flatten'))(out_roi_pool)\n",
    "    out = TimeDistributed(Dense(4096, activation='relu', name='fc1'))(out)\n",
    "    out = TimeDistributed(Dropout(0.5))(out)\n",
    "    out = TimeDistributed(Dense(4096, activation='relu', name='fc2'))(out)\n",
    "    out = TimeDistributed(Dropout(0.5))(out)\n",
    "\n",
    "    # There are two output layer\n",
    "    # out_class: softmax acivation function for classify the class name of the object\n",
    "    \n",
    "    # out_regr: linear activation function for bboxes coordinates regression\n",
    "    out_class = TimeDistributed(Dense(nb_classes, activation='softmax', kernel_initializer='zero'), name='dense_class_{}'.format(nb_classes))(out)\n",
    "    \n",
    "    # note: no regression target for bg class\n",
    "    out_regr = TimeDistributed(Dense(4 * (nb_classes-1), activation='linear', kernel_initializer='zero'), name='dense_regress_{}'.format(nb_classes))(out)\n",
    "\n",
    "    return [out_class, out_regr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intersection of Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def union(au, bu, area_intersection):\n",
    "    area_a = (au[2] - au[0]) * (au[3] - au[1])\n",
    "    area_b = (bu[2] - bu[0]) * (bu[3] - bu[1])\n",
    "    area_union = area_a + area_b - area_intersection\n",
    "    return area_union\n",
    "\n",
    "\n",
    "def intersection(ai, bi):\n",
    "    x = max(ai[0], bi[0])\n",
    "    y = max(ai[1], bi[1])\n",
    "    w = min(ai[2], bi[2]) - x\n",
    "    h = min(ai[3], bi[3]) - y\n",
    "    if w < 0 or h < 0:\n",
    "        return 0\n",
    "    return w*h\n",
    "\n",
    "\n",
    "def iou(a, b):\n",
    "    # a and b should be (x1,y1,x2,y2)\n",
    "\n",
    "    if a[0] >= a[2] or a[1] >= a[3] or b[0] >= b[2] or b[1] >= b[3]:\n",
    "        return 0.0\n",
    "\n",
    "    area_i = intersection(a, b)\n",
    "    area_u = union(a, b, area_i)\n",
    "\n",
    "    return float(area_i) / float(area_u + 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RPN for all anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rpn(C, img_data, width, height, resized_width, resized_height, img_length_calc_function):\n",
    "    \n",
    "    downscale = float(C.rpn_stride) \n",
    "    anchor_sizes = C.anchor_box_scales   # 128, 256, 512\n",
    "    \n",
    "    anchor_ratios = C.anchor_box_ratios  # 1:1, 1:2*sqrt(2), 2*sqrt(2):1\n",
    "    \n",
    "    num_anchors = len(anchor_sizes) * len(anchor_ratios) # 3x3=9\n",
    "\n",
    "    # calculate the output map size based on the network architecture\n",
    "    (output_width, output_height) = img_length_calc_function(resized_width, resized_height)\n",
    "\n",
    "    n_anchratios = len(anchor_ratios)    # 3\n",
    "    \n",
    "    # initialise empty output objectives\n",
    "    y_rpn_overlap = np.zeros((output_height, output_width, num_anchors))\n",
    "    y_is_box_valid = np.zeros((output_height, output_width, num_anchors))\n",
    "    y_rpn_regr = np.zeros((output_height, output_width, num_anchors * 4))\n",
    "\n",
    "    num_bboxes = len(img_data['bboxes'])\n",
    "\n",
    "    num_anchors_for_bbox = np.zeros(num_bboxes).astype(int)\n",
    "    best_anchor_for_bbox = -1*np.ones((num_bboxes, 4)).astype(int)\n",
    "    best_iou_for_bbox = np.zeros(num_bboxes).astype(np.float32)\n",
    "    best_x_for_bbox = np.zeros((num_bboxes, 4)).astype(int)\n",
    "    best_dx_for_bbox = np.zeros((num_bboxes, 4)).astype(np.float32)\n",
    "\n",
    "    # get the GT box coordinates, and resize to account for image resizing\n",
    "    gta = np.zeros((num_bboxes, 4))\n",
    "    for bbox_num, bbox in enumerate(img_data['bboxes']):\n",
    "        # get the GT box coordinates, and resize to account for image resizing\n",
    "        gta[bbox_num, 0] = bbox['x1'] * (resized_width / float(width))\n",
    "        gta[bbox_num, 1] = bbox['x2'] * (resized_width / float(width))\n",
    "        gta[bbox_num, 2] = bbox['y1'] * (resized_height / float(height))\n",
    "        gta[bbox_num, 3] = bbox['y2'] * (resized_height / float(height))\n",
    "    \n",
    "    # rpn ground truth\n",
    "\n",
    "    for anchor_size_idx in range(len(anchor_sizes)):\n",
    "        for anchor_ratio_idx in range(n_anchratios):\n",
    "            anchor_x = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][0]\n",
    "            anchor_y = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][1]\t\n",
    "            \n",
    "            for ix in range(output_width):\t\t\t\t\t\n",
    "                # x-coordinates of the current anchor box\t\n",
    "                x1_anc = downscale * (ix + 0.5) - anchor_x / 2\n",
    "                x2_anc = downscale * (ix + 0.5) + anchor_x / 2\t\n",
    "                \n",
    "                # ignore boxes that go across image boundaries\t\t\t\t\t\n",
    "                if x1_anc < 0 or x2_anc > resized_width:\n",
    "                    continue\n",
    "                    \n",
    "                for jy in range(output_height):\n",
    "                    \n",
    "                    # y-coordinates of the current anchor box\n",
    "                    y1_anc = downscale * (jy + 0.5) - anchor_y / 2\n",
    "                    y2_anc = downscale * (jy + 0.5) + anchor_y / 2\n",
    "                    \n",
    "                    # ignore boxes that go across image boundaries\n",
    "                    if y1_anc < 0 or y2_anc > resized_height:\n",
    "                        continue\n",
    "\n",
    "                    # bbox_type indicates whether an anchor should be a target\n",
    "                    # Initialize with 'negative'\n",
    "                    bbox_type = 'neg'\n",
    "\n",
    "                    # this is the best IOU for the (x,y) coord and the current anchor\n",
    "                    # note that this is different from the best IOU for a GT bbox\n",
    "                    best_iou_for_loc = 0.0\n",
    "                    \n",
    "                    for bbox_num in range(num_bboxes):\n",
    "                        \n",
    "                        # get IOU of the current GT box and the current anchor box\n",
    "                        curr_iou = iou([gta[bbox_num, 0], gta[bbox_num, 2], gta[bbox_num, 1], gta[bbox_num, 3]], [x1_anc, y1_anc, x2_anc, y2_anc])\n",
    "                        # calculate the regression targets if they will be needed\n",
    "                        if curr_iou > best_iou_for_bbox[bbox_num] or curr_iou > C.rpn_max_overlap:\n",
    "                            cx = (gta[bbox_num, 0] + gta[bbox_num, 1]) / 2.0\n",
    "                            cy = (gta[bbox_num, 2] + gta[bbox_num, 3]) / 2.0\n",
    "                            cxa = (x1_anc + x2_anc)/2.0\n",
    "                            cya = (y1_anc + y2_anc)/2.0\n",
    "                            # x,y are the center point of ground-truth bbox\n",
    "                            # xa,ya are the center point of anchor bbox (xa=downscale * (ix + 0.5); ya=downscale * (iy+0.5))\n",
    "                            # w,h are the width and height of ground-truth bbox\n",
    "                            # wa,ha are the width and height of anchor bboxe\n",
    "                            # tx = (x - xa) / wa\n",
    "                            # ty = (y - ya) / ha\n",
    "                            # tw = log(w / wa)\n",
    "                            # th = log(h / ha)\n",
    "                            tx = (cx - cxa) / (x2_anc - x1_anc)\n",
    "                            ty = (cy - cya) / (y2_anc - y1_anc)\n",
    "                            tw = np.log((gta[bbox_num, 1] - gta[bbox_num, 0]) / (x2_anc - x1_anc))\n",
    "                            th = np.log((gta[bbox_num, 3] - gta[bbox_num, 2]) / (y2_anc - y1_anc))\n",
    "                        \n",
    "                        if img_data['bboxes'][bbox_num]['class'] != 'bg':\n",
    "                            \n",
    "                            # all GT boxes should be mapped to an anchor box, so we keep track of which anchor box was best\n",
    "                            if curr_iou > best_iou_for_bbox[bbox_num]:\n",
    "                                best_anchor_for_bbox[bbox_num] = [jy, ix, anchor_ratio_idx, anchor_size_idx]\n",
    "                                best_iou_for_bbox[bbox_num] = curr_iou\n",
    "                                best_x_for_bbox[bbox_num,:] = [x1_anc, x2_anc, y1_anc, y2_anc]\n",
    "                                best_dx_for_bbox[bbox_num,:] = [tx, ty, tw, th]\n",
    "\n",
    "                            # we set the anchor to positive if the IOU is >0.7 (it does not matter if there was another better box, it just indicates overlap)\n",
    "                            if curr_iou > C.rpn_max_overlap:\n",
    "                                bbox_type = 'pos'\n",
    "                                num_anchors_for_bbox[bbox_num] += 1\n",
    "                                # we update the regression layer target if this IOU is the best for the current (x,y) and anchor position\n",
    "                                if curr_iou > best_iou_for_loc:\n",
    "                                    best_iou_for_loc = curr_iou\n",
    "                                    best_regr = (tx, ty, tw, th)\n",
    "\n",
    "                            # if the IOU is >0.3 and <0.7, it is ambiguous and no included in the objective\n",
    "                            if C.rpn_min_overlap < curr_iou < C.rpn_max_overlap:\n",
    "                                # gray zone between neg and pos\n",
    "                                if bbox_type != 'pos':\n",
    "                                    bbox_type = 'neutral'\n",
    "\n",
    "                    # turn on or off outputs depending on IOUs\n",
    "                    if bbox_type == 'neg':\n",
    "                        y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n",
    "                        y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\n",
    "                    elif bbox_type == 'neutral':\n",
    "                        y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\n",
    "                        y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\n",
    "                    elif bbox_type == 'pos':\n",
    "                        y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n",
    "                        y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n",
    "                        start = 4 * (anchor_ratio_idx + n_anchratios * anchor_size_idx)\n",
    "                        y_rpn_regr[jy, ix, start:start+4] = best_regr\n",
    "\n",
    "    # we ensure that every bbox has at least one positive RPN region\n",
    "\n",
    "    for idx in range(num_anchors_for_bbox.shape[0]):\n",
    "        if num_anchors_for_bbox[idx] == 0:\n",
    "            # no box with an IOU greater than zero ...\n",
    "            if best_anchor_for_bbox[idx, 0] == -1:\n",
    "                continue\n",
    "            y_is_box_valid[\n",
    "                best_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], best_anchor_for_bbox[idx,2] + n_anchratios *\n",
    "                best_anchor_for_bbox[idx,3]] = 1\n",
    "            y_rpn_overlap[\n",
    "                best_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], best_anchor_for_bbox[idx,2] + n_anchratios *\n",
    "                best_anchor_for_bbox[idx,3]] = 1\n",
    "            start = 4 * (best_anchor_for_bbox[idx,2] + n_anchratios * best_anchor_for_bbox[idx,3])\n",
    "            y_rpn_regr[\n",
    "                best_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], start:start+4] = best_dx_for_bbox[idx, :]\n",
    "\n",
    "    y_rpn_overlap = np.transpose(y_rpn_overlap, (2, 0, 1))\n",
    "    y_rpn_overlap = np.expand_dims(y_rpn_overlap, axis=0)\n",
    "    \n",
    "    y_is_box_valid = np.transpose(y_is_box_valid, (2, 0, 1))\n",
    "    y_is_box_valid = np.expand_dims(y_is_box_valid, axis=0)\n",
    "\n",
    "    y_rpn_regr = np.transpose(y_rpn_regr, (2, 0, 1))\n",
    "    y_rpn_regr = np.expand_dims(y_rpn_regr, axis=0)\n",
    "\n",
    "    pos_locs = np.where(np.logical_and(y_rpn_overlap[0, :, :, :] == 1, y_is_box_valid[0, :, :, :] == 1))\n",
    "    neg_locs = np.where(np.logical_and(y_rpn_overlap[0, :, :, :] == 0, y_is_box_valid[0, :, :, :] == 1))\n",
    "\n",
    "    num_pos = len(pos_locs[0])\n",
    "\n",
    "    # one issue is that the RPN has many more negative than positive regions, so we turn off some of the negative\n",
    "    # regions. We also limit it to 256 regions.\n",
    "    num_regions = 256\n",
    "\n",
    "    if len(pos_locs[0]) > num_regions/2:\n",
    "        val_locs = random.sample(range(len(pos_locs[0])), len(pos_locs[0]) - num_regions/2)\n",
    "        y_is_box_valid[0, pos_locs[0][val_locs], pos_locs[1][val_locs], pos_locs[2][val_locs]] = 0\n",
    "        num_pos = num_regions/2\n",
    "\n",
    "    if len(neg_locs[0]) + num_pos > num_regions:\n",
    "        val_locs = random.sample(range(len(neg_locs[0])), len(neg_locs[0]) - num_pos)\n",
    "        y_is_box_valid[0, neg_locs[0][val_locs], neg_locs[1][val_locs], neg_locs[2][val_locs]] = 0\n",
    "\n",
    "    y_rpn_cls = np.concatenate([y_is_box_valid, y_rpn_overlap], axis=1)\n",
    "    y_rpn_regr = np.concatenate([np.repeat(y_rpn_overlap, 4, axis=1), y_rpn_regr], axis=1)\n",
    "\n",
    "    return np.copy(y_rpn_cls), np.copy(y_rpn_regr), num_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get new image and Augment it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_new_img_size(width, height, img_min_side=300):\n",
    "    if width <= height:\n",
    "        f = float(img_min_side) / width\n",
    "        resized_height = int(f * height)\n",
    "        resized_width = img_min_side\n",
    "    else:\n",
    "        f = float(img_min_side) / height\n",
    "        resized_width = int(f * width)\n",
    "        resized_height = img_min_side\n",
    "\n",
    "    return resized_width, resized_height\n",
    "\n",
    "def augment(img_data, config, augment=True):\n",
    "    assert 'filepath' in img_data\n",
    "    assert 'bboxes' in img_data\n",
    "    assert 'width' in img_data\n",
    "    assert 'height' in img_data\n",
    "\n",
    "    img_data_aug = copy.deepcopy(img_data)\n",
    "\n",
    "    img = cv2.imread(img_data_aug['filepath'])\n",
    "\n",
    "    if augment:\n",
    "        rows, cols = img.shape[:2]\n",
    "        \n",
    "        if config.use_horizontal_flips and np.random.randint(0, 2) == 0:\n",
    "            img = cv2.flip(img, 1)\n",
    "            for bbox in img_data_aug['bboxes']:\n",
    "                x1 = bbox['x1']\n",
    "                x2 = bbox['x2']\n",
    "                bbox['x2'] = cols - x1\n",
    "                bbox['x1'] = cols - x2\n",
    "\n",
    "        if config.use_vertical_flips and np.random.randint(0, 2) == 0:\n",
    "            img = cv2.flip(img, 0)\n",
    "            for bbox in img_data_aug['bboxes']:\n",
    "                y1 = bbox['y1']\n",
    "                y2 = bbox['y2']\n",
    "                bbox['y2'] = rows - y1\n",
    "                bbox['y1'] = rows - y2\n",
    "\n",
    "        if config.rot_90:\n",
    "            angle = np.random.choice([0,90,180,270],1)[0]\n",
    "            if angle == 270:\n",
    "                img = np.transpose(img, (1,0,2))\n",
    "                img = cv2.flip(img, 0)\n",
    "            elif angle == 180:\n",
    "                img = cv2.flip(img, -1)\n",
    "            elif angle == 90:\n",
    "                img = np.transpose(img, (1,0,2))\n",
    "                img = cv2.flip(img, 1)\n",
    "            elif angle == 0:\n",
    "                pass\n",
    "\n",
    "            for bbox in img_data_aug['bboxes']:\n",
    "                x1 = bbox['x1']\n",
    "                x2 = bbox['x2']\n",
    "                y1 = bbox['y1']\n",
    "                y2 = bbox['y2']\n",
    "                if angle == 270:\n",
    "                    bbox['x1'] = y1\n",
    "                    bbox['x2'] = y2\n",
    "                    bbox['y1'] = cols - x2\n",
    "                    bbox['y2'] = cols - x1\n",
    "                elif angle == 180:\n",
    "                    bbox['x2'] = cols - x1\n",
    "                    bbox['x1'] = cols - x2\n",
    "                    bbox['y2'] = rows - y1\n",
    "                    bbox['y1'] = rows - y2\n",
    "                elif angle == 90:\n",
    "                    bbox['x1'] = rows - y2\n",
    "                    bbox['x2'] = rows - y1\n",
    "                    bbox['y1'] = x1\n",
    "                    bbox['y2'] = x2        \n",
    "                elif angle == 0:\n",
    "                    pass\n",
    "\n",
    "    img_data_aug['width'] = img.shape[1]\n",
    "    img_data_aug['height'] = img.shape[0]\n",
    "    return img_data_aug, img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geerate Ground truth anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anchor_gt(all_img_data, C, img_length_calc_function, mode='train'):\n",
    "\n",
    "    while True:\n",
    "\n",
    "        for img_data in all_img_data:\n",
    "            try:\n",
    "\n",
    "                # read in image, and optionally add augmentation\n",
    "                \n",
    "                if mode == 'train':\n",
    "                    img_data_aug, x_img = augment(img_data, C, augment=True)\n",
    "                else:\n",
    "                    img_data_aug, x_img = augment(img_data, C, augment=False)\n",
    "\n",
    "                (width, height) = (img_data_aug['width'], img_data_aug['height'])\n",
    "                (rows, cols, _) = x_img.shape\n",
    "\n",
    "                assert cols == width\n",
    "                assert rows == height\n",
    "\n",
    "                # get image dimensions for resizing\n",
    "                (resized_width, resized_height) = get_new_img_size(width, height, C.im_size)\n",
    "\n",
    "                # resize the image so that smalles side is length = 300px\n",
    "                x_img = cv2.resize(x_img, (resized_width, resized_height), interpolation=cv2.INTER_CUBIC)\n",
    "                debug_img = x_img.copy()\n",
    "\n",
    "                try:\n",
    "                    y_rpn_cls, y_rpn_regr, num_pos = calc_rpn(C, img_data_aug, width, height, resized_width, resized_height, img_length_calc_function)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                # Zero-center by mean pixel, and preprocess image\n",
    "\n",
    "                x_img = x_img[:,:, (2, 1, 0)]  # BGR -> RGB\n",
    "                x_img = x_img.astype(np.float32)\n",
    "                x_img[:, :, 0] -= C.img_channel_mean[0]\n",
    "                x_img[:, :, 1] -= C.img_channel_mean[1]\n",
    "                x_img[:, :, 2] -= C.img_channel_mean[2]\n",
    "                x_img /= C.img_scaling_factor\n",
    "\n",
    "                x_img = np.transpose(x_img, (2, 0, 1))\n",
    "                x_img = np.expand_dims(x_img, axis=0)\n",
    "\n",
    "                y_rpn_regr[:, y_rpn_regr.shape[1]//2:, :, :] *= C.std_scaling\n",
    "\n",
    "                x_img = np.transpose(x_img, (0, 2, 3, 1))\n",
    "                y_rpn_cls = np.transpose(y_rpn_cls, (0, 2, 3, 1))\n",
    "                y_rpn_regr = np.transpose(y_rpn_regr, (0, 2, 3, 1))\n",
    "\n",
    "                yield np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug, debug_img, num_pos\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def non_max_suppression_fast(boxes, probs, overlap_thresh=0.9, max_boxes=300):\n",
    "    # code used from here: http://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/\n",
    "    # if there are no boxes, return an empty list\n",
    "  \n",
    "    # Process explanation:\n",
    "    #   Step 1: Sort the probs list\n",
    "    #   Step 2: Find the larget prob 'Last' in the list and save it to the pick list\n",
    "    #   Step 3: Calculate the IoU with 'Last' box and other boxes in the list. If the IoU is larger than overlap_threshold, delete the box from list\n",
    "    #   Step 4: Repeat step 2 and step 3 until there is no item in the probs list \n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    # grab the coordinates of the bounding boxes\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "\n",
    "    np.testing.assert_array_less(x1, x2)\n",
    "    np.testing.assert_array_less(y1, y2)\n",
    "\n",
    "    # if the bounding boxes integers, convert them to floats --\n",
    "    # this is important since we'll be doing a bunch of divisions\n",
    "    if boxes.dtype.kind == \"i\":\n",
    "        boxes = boxes.astype(\"float\")\n",
    "\n",
    "    # initialize the list of picked indexes\t\n",
    "    pick = []\n",
    "\n",
    "    # calculate the areas\n",
    "    area = (x2 - x1) * (y2 - y1)\n",
    "\n",
    "    # sort the bounding boxes \n",
    "    idxs = np.argsort(probs)\n",
    "    \n",
    "    # keep looping while some indexes still remain in the indexes\n",
    "    # list\n",
    "    while len(idxs) > 0:\n",
    "        # grab the last index in the indexes list and add the\n",
    "        # index value to the list of picked indexes\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i)\n",
    "\n",
    "        # find the intersection\n",
    "\n",
    "        xx1_int = np.maximum(x1[i], x1[idxs[:last]])\n",
    "        yy1_int = np.maximum(y1[i], y1[idxs[:last]])\n",
    "        xx2_int = np.minimum(x2[i], x2[idxs[:last]])\n",
    "        yy2_int = np.minimum(y2[i], y2[idxs[:last]])\n",
    "        \n",
    "        ww_int = np.maximum(0, xx2_int - xx1_int)\n",
    "        hh_int = np.maximum(0, yy2_int - yy1_int)\n",
    "\n",
    "        area_int = ww_int * hh_int\n",
    "        \n",
    "        # find the union\n",
    "        area_union = area[i] + area[idxs[:last]] - area_int\n",
    "\n",
    "        # compute the ratio of overlap\n",
    "        overlap = area_int/(area_union + 1e-6)\n",
    "\n",
    "        # delete all indexes from the index list that have\n",
    "        idxs = np.delete(idxs, np.concatenate(([last],\n",
    "            np.where(overlap > overlap_thresh)[0])))\n",
    "\n",
    "        if len(pick) >= max_boxes:\n",
    "            break\n",
    "\n",
    "    # return only the bounding boxes that were picked using the integer data type\n",
    "    boxes = boxes[pick].astype(\"int\")\n",
    "    probs = probs[pick]\n",
    "    return boxes, probs\n",
    "\n",
    "def apply_regr_np(X, T):\n",
    "\n",
    "    try:\n",
    "        x = X[0, :, :]\n",
    "        y = X[1, :, :]\n",
    "        w = X[2, :, :]\n",
    "        h = X[3, :, :]\n",
    "\n",
    "        tx = T[0, :, :]\n",
    "        ty = T[1, :, :]\n",
    "        tw = T[2, :, :]\n",
    "        th = T[3, :, :]\n",
    "        cx = x + w/2.\n",
    "        cy = y + h/2.\n",
    "        cx1 = tx * w + cx\n",
    "        cy1 = ty * h + cy\n",
    "        \n",
    "        w1 = np.exp(tw.astype(np.float64)) * w\n",
    "        h1 = np.exp(th.astype(np.float64)) * h\n",
    "        x1 = cx1 - w1/2.\n",
    "        y1 = cy1 - h1/2.\n",
    "\n",
    "        x1 = np.round(x1)\n",
    "        y1 = np.round(y1)\n",
    "        w1 = np.round(w1)\n",
    "        h1 = np.round(h1)\n",
    "        return np.stack([x1, y1, w1, h1])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return X\n",
    "    \n",
    "def apply_regr(x, y, w, h, tx, ty, tw, th):\n",
    "    # Apply regression to x, y, w and h\n",
    "    try:\n",
    "        cx = x + w/2.\n",
    "        cy = y + h/2.\n",
    "        cx1 = tx * w + cx\n",
    "        cy1 = ty * h + cy\n",
    "        w1 = math.exp(tw) * w\n",
    "        h1 = math.exp(th) * h\n",
    "        x1 = cx1 - w1/2.\n",
    "        y1 = cy1 - h1/2.\n",
    "        x1 = int(round(x1))\n",
    "        y1 = int(round(y1))\n",
    "        w1 = int(round(w1))\n",
    "        h1 = int(round(h1))\n",
    "        \n",
    "        return x1, y1, w1, h1\n",
    "\n",
    "    except ValueError:\n",
    "        return x, y, w, h\n",
    "    except OverflowError:\n",
    "        return x, y, w, h\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return x, y, w, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RPN to ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rpn_to_roi(rpn_layer, regr_layer, C, dim_ordering, use_regr=True, max_boxes=300,overlap_thresh=0.9):\n",
    "    \"\"\"Convert rpn layer to roi bboxes\n",
    "\n",
    "    Args: (num_anchors = 9)\n",
    "        rpn_layer: output layer for rpn classification \n",
    "            shape (1, feature_map.height, feature_map.width, num_anchors)\n",
    "            Might be (1, 18, 25, 9) if resized image is 400 width and 300\n",
    "        regr_layer: output layer for rpn regression\n",
    "            shape (1, feature_map.height, feature_map.width, num_anchors)\n",
    "            Might be (1, 18, 25, 36) if resized image is 400 width and 300\n",
    "        C: config\n",
    "        use_regr: Wether to use bboxes regression in rpn\n",
    "        max_boxes: max bboxes number for non-max-suppression (NMS)\n",
    "        overlap_thresh: If iou in NMS is larger than this threshold, drop the box\n",
    "\n",
    "    Returns:\n",
    "        result: boxes from non-max-suppression (shape=(300, 4))\n",
    "            boxes: coordinates for bboxes (on the feature map)\n",
    "    \"\"\"\n",
    "    regr_layer = regr_layer / C.std_scaling\n",
    "\n",
    "    anchor_sizes = C.anchor_box_scales   # (3 in here)\n",
    "    anchor_ratios = C.anchor_box_ratios  # (3 in here)\n",
    "\n",
    "    assert rpn_layer.shape[0] == 1\n",
    "\n",
    "    (rows, cols) = rpn_layer.shape[1:3]\n",
    "\n",
    "    curr_layer = 0\n",
    "\n",
    "    # A.shape = (4, feature_map.height, feature_map.width, num_anchors) \n",
    "    # Might be (4, 18, 25, 9) if resized image is 400 width and 300\n",
    "    # A is the coordinates for 9 anchors for every point in the feature map \n",
    "    # => all 18x25x9=4050 anchors cooridnates\n",
    "    A = np.zeros((4, rpn_layer.shape[1], rpn_layer.shape[2], rpn_layer.shape[3]))\n",
    "\n",
    "    for anchor_size in anchor_sizes:\n",
    "        for anchor_ratio in anchor_ratios:\n",
    "            # anchor_x = (128 * 1) / 16 = 8  => width of current anchor\n",
    "            # anchor_y = (128 * 2) / 16 = 16 => height of current anchor\n",
    "            anchor_x = (anchor_size * anchor_ratio[0])/C.rpn_stride\n",
    "            anchor_y = (anchor_size * anchor_ratio[1])/C.rpn_stride\n",
    "            \n",
    "            # curr_layer: 0~8 (9 anchors)\n",
    "            # the Kth anchor of all position in the feature map (9th in total)\n",
    "            regr = regr_layer[0, :, :, 4 * curr_layer:4 * curr_layer + 4] # shape => (18, 25, 4)\n",
    "            regr = np.transpose(regr, (2, 0, 1)) # shape => (4, 18, 25)\n",
    "\n",
    "            # Create 18x25 mesh grid\n",
    "            # For every point in x, there are all the y points and vice versa\n",
    "            # X.shape = (18, 25)\n",
    "            # Y.shape = (18, 25)\n",
    "            X, Y = np.meshgrid(np.arange(cols),np. arange(rows))\n",
    "\n",
    "            # Calculate anchor position and size for each feature map point\n",
    "            A[0, :, :, curr_layer] = X - anchor_x/2 # Top left x coordinate\n",
    "            A[1, :, :, curr_layer] = Y - anchor_y/2 # Top left y coordinate\n",
    "            A[2, :, :, curr_layer] = anchor_x       # width of current anchor\n",
    "            A[3, :, :, curr_layer] = anchor_y       # height of current anchor\n",
    "\n",
    "            # Apply regression to x, y, w and h if there is rpn regression layer\n",
    "            if use_regr:\n",
    "                A[:, :, :, curr_layer] = apply_regr_np(A[:, :, :, curr_layer], regr)\n",
    "\n",
    "            # Avoid width and height exceeding 1\n",
    "            A[2, :, :, curr_layer] = np.maximum(1, A[2, :, :, curr_layer])\n",
    "            A[3, :, :, curr_layer] = np.maximum(1, A[3, :, :, curr_layer])\n",
    "\n",
    "            # Convert (x, y , w, h) to (x1, y1, x2, y2)\n",
    "            # x1, y1 is top left coordinate\n",
    "            # x2, y2 is bottom right coordinate\n",
    "            A[2, :, :, curr_layer] += A[0, :, :, curr_layer]\n",
    "            A[3, :, :, curr_layer] += A[1, :, :, curr_layer]\n",
    "\n",
    "            # Avoid bboxes drawn outside the feature map\n",
    "            A[0, :, :, curr_layer] = np.maximum(0, A[0, :, :, curr_layer])\n",
    "            A[1, :, :, curr_layer] = np.maximum(0, A[1, :, :, curr_layer])\n",
    "            A[2, :, :, curr_layer] = np.minimum(cols-1, A[2, :, :, curr_layer])\n",
    "            A[3, :, :, curr_layer] = np.minimum(rows-1, A[3, :, :, curr_layer])\n",
    "\n",
    "            curr_layer += 1\n",
    "    all_boxes = np.reshape(A.transpose((0, 3, 1, 2)), (4, -1)).transpose((1, 0))  # shape=(4050, 4)\n",
    "    all_probs = rpn_layer.transpose((0, 3, 1, 2)).reshape((-1))                   # shape=(4050,)\n",
    "\n",
    "    x1 = all_boxes[:, 0]\n",
    "    y1 = all_boxes[:, 1]\n",
    "    x2 = all_boxes[:, 2]\n",
    "    y2 = all_boxes[:, 3]\n",
    "\n",
    "    # Find out the bboxes which is illegal and delete them from bboxes list\n",
    "    idxs = np.where((x1 - x2 >= 0) | (y1 - y2 >= 0))\n",
    "\n",
    "    all_boxes = np.delete(all_boxes, idxs, 0)\n",
    "    all_probs = np.delete(all_probs, idxs, 0)\n",
    "\n",
    "    # Apply non_max_suppression\n",
    "    # Only extract the bboxes. Don't need rpn probs in the later process\n",
    "    result = non_max_suppression_fast(all_boxes, all_probs, overlap_thresh=overlap_thresh, max_boxes=max_boxes)[0]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'New attempt\\Images'\n",
    "\n",
    "test_path = 'New attempt\\Images\\annotations' # Test data (annotation file)\n",
    "\n",
    "test_base_path = 'New attempt\\Images\\test' # Directory to save the test images\n",
    "\n",
    "config_output_filename = os.path.join(base_path, 'model_vgg_config.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'New attempt\\\\model_vgg_config.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-c4c02e751ad3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_output_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf_in\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_in\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# turn off any data augmentation at test time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_horizontal_flips\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'New attempt\\\\model_vgg_config.pickle'"
     ]
    }
   ],
   "source": [
    "with open(config_output_filename, 'rb') as f_in:\n",
    "    C = pickle.load(f_in)\n",
    "\n",
    "# turn off any data augmentation at test time\n",
    "C.use_horizontal_flips = False\n",
    "C.use_vertical_flips = False\n",
    "C.rot_90 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_img_size(img, C):\n",
    "    \"\"\" formats the image size based on config \"\"\"\n",
    "    img_min_side = float(C.im_size)\n",
    "    (height,width,_) = img.shape\n",
    "        \n",
    "    if width <= height:\n",
    "        ratio = img_min_side/width\n",
    "        new_height = int(ratio * height)\n",
    "        new_width = int(img_min_side)\n",
    "    else:\n",
    "        ratio = img_min_side/height\n",
    "        new_width = int(ratio * width)\n",
    "        new_height = int(img_min_side)\n",
    "    img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n",
    "    return img, ratio\t\n",
    "\n",
    "def format_img_channels(img, C):\n",
    "    \"\"\" formats the image channels based on config \"\"\"\n",
    "    img = img[:, :, (2, 1, 0)]\n",
    "    img = img.astype(np.float32)\n",
    "    img[:, :, 0] -= C.img_channel_mean[0]\n",
    "    img[:, :, 1] -= C.img_channel_mean[1]\n",
    "    img[:, :, 2] -= C.img_channel_mean[2]\n",
    "    img /= C.img_scaling_factor\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "def format_img(img, C):\n",
    "    \"\"\" formats an image for model prediction based on config \"\"\"\n",
    "    img, ratio = format_img_size(img, C)\n",
    "    img = format_img_channels(img, C)\n",
    "    return img, ratio\n",
    "\n",
    "# Method to transform the coordinates of the bounding box to its original size\n",
    "def get_real_coordinates(ratio, x1, y1, x2, y2):\n",
    "\n",
    "    real_x1 = int(round(x1 // ratio))\n",
    "    real_y1 = int(round(y1 // ratio))\n",
    "    real_x2 = int(round(x2 // ratio))\n",
    "    real_y2 = int(round(y2 // ratio))\n",
    "\n",
    "    return (real_x1, real_y1, real_x2 ,real_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-7a86ca85127a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mimg_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_shape_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mroi_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_rois\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mfeature_map_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_shape_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'C' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "num_features = 512\n",
    "\n",
    "input_shape_img = (None, None, 3)\n",
    "input_shape_features = (None, None, num_features)\n",
    "\n",
    "img_input = Input(shape=input_shape_img)\n",
    "roi_input = Input(shape=(C.num_rois, 4))\n",
    "feature_map_input = Input(shape=input_shape_features)\n",
    "\n",
    "# define the base network (VGG here, can be Resnet50, Inception, etc)\n",
    "shared_layers = nn_base(img_input, trainable=True)\n",
    "\n",
    "# define the RPN, built on the base layers\n",
    "num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)\n",
    "rpn_layers = rpn_layer(shared_layers, num_anchors)\n",
    "\n",
    "classifier = classifier_layer(feature_map_input, roi_input, C.num_rois, nb_classes=len(C.class_mapping))\n",
    "\n",
    "model_rpn = Model(img_input, rpn_layers)\n",
    "model_classifier_only = Model([feature_map_input, roi_input], classifier)\n",
    "\n",
    "model_classifier = Model([feature_map_input, roi_input], classifier)\n",
    "\n",
    "print('Loading weights from {}'.format(C.model_path))\n",
    "model_rpn.load_weights(C.model_path, by_name=True)\n",
    "\n",
    "model_classifier.load_weights(C.model_path, by_name=True)\n",
    "\n",
    "model_rpn.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "model_classifier.compile(optimizer='sgd', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-7a57148bd522>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Switch key value for class mapping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mclass_mapping\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_mapping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mclass_mapping\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclass_mapping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_mapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mclass_to_color\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mclass_mapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclass_mapping\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'C' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Switch key value for class mapping\n",
    "class_mapping = C.class_mapping\n",
    "class_mapping = {v: k for k, v in class_mapping.items()}\n",
    "print(class_mapping)\n",
    "class_to_color = {class_mapping[v]: np.random.randint(0, 255, 3) for v in class_mapping}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 123] The filename, directory name, or volume label syntax is incorrect: 'ImageAI\\test01'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-646bc48720e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_imgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_base_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mimgs_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_imgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 123] The filename, directory name, or volume label syntax is incorrect: 'ImageAI\\test01'"
     ]
    }
   ],
   "source": [
    "test_imgs = os.listdir(test_base_path)\n",
    "\n",
    "imgs_path = []\n",
    "for i in range(12):\n",
    "    idx = np.random.randint(len(test_imgs))\n",
    "    imgs_path.append(test_imgs[idx])\n",
    "\n",
    "all_imgs = []\n",
    "\n",
    "classes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imgs_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-034bda10c106>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbbox_threshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.7\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mimg_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.bmp'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'.jpeg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'.jpg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'.png'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'.tif'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'.tiff'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'imgs_path' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# If the box classification value is less than this, we ignore this box\n",
    "bbox_threshold = 0.7\n",
    "\n",
    "for idx, img_name in enumerate(imgs_path):\n",
    "    if not img_name.lower().endswith(('.bmp', '.jpeg', '.jpg', '.png', '.tif', '.tiff')):\n",
    "        continue\n",
    "    print(img_name)\n",
    "    st = time.time()\n",
    "    filepath = os.path.join(test_base_path, img_name)\n",
    "\n",
    "    img = cv2.imread(filepath)\n",
    "\n",
    "    X, ratio = format_img(img, C)\n",
    "    \n",
    "    X = np.transpose(X, (0, 2, 3, 1))\n",
    "\n",
    "    # get output layer Y1, Y2 from the RPN and the feature maps F\n",
    "    # Y1: y_rpn_cls\n",
    "    # Y2: y_rpn_regr\n",
    "    [Y1, Y2, F] = model_rpn.predict(X)\n",
    "\n",
    "    # Get bboxes by applying NMS \n",
    "    # R.shape = (300, 4)\n",
    "    R = rpn_to_roi(Y1, Y2, C, K.image_dim_ordering(), overlap_thresh=0.7)\n",
    "\n",
    "    # convert from (x1,y1,x2,y2) to (x,y,w,h)\n",
    "    R[:, 2] -= R[:, 0]\n",
    "    R[:, 3] -= R[:, 1]\n",
    "\n",
    "    # apply the spatial pyramid pooling to the proposed regions\n",
    "    bboxes = {}\n",
    "    probs = {}\n",
    "\n",
    "    for jk in range(R.shape[0]//C.num_rois + 1):\n",
    "        ROIs = np.expand_dims(R[C.num_rois*jk:C.num_rois*(jk+1), :], axis=0)\n",
    "        if ROIs.shape[1] == 0:\n",
    "            break\n",
    "\n",
    "        if jk == R.shape[0]//C.num_rois:\n",
    "            #pad R\n",
    "            curr_shape = ROIs.shape\n",
    "            target_shape = (curr_shape[0],C.num_rois,curr_shape[2])\n",
    "            ROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)\n",
    "            ROIs_padded[:, :curr_shape[1], :] = ROIs\n",
    "            ROIs_padded[0, curr_shape[1]:, :] = ROIs[0, 0, :]\n",
    "            ROIs = ROIs_padded\n",
    "\n",
    "        [P_cls, P_regr] = model_classifier_only.predict([F, ROIs])\n",
    "\n",
    "        # Calculate bboxes coordinates on resized image\n",
    "        for ii in range(P_cls.shape[1]):\n",
    "            # Ignore 'bg' class\n",
    "            if np.max(P_cls[0, ii, :]) < bbox_threshold or np.argmax(P_cls[0, ii, :]) == (P_cls.shape[2] - 1):\n",
    "                continue\n",
    "\n",
    "            cls_name = class_mapping[np.argmax(P_cls[0, ii, :])]\n",
    "\n",
    "            if cls_name not in bboxes:\n",
    "                bboxes[cls_name] = []\n",
    "                probs[cls_name] = []\n",
    "\n",
    "            (x, y, w, h) = ROIs[0, ii, :]\n",
    "\n",
    "            cls_num = np.argmax(P_cls[0, ii, :])\n",
    "            try:\n",
    "                (tx, ty, tw, th) = P_regr[0, ii, 4*cls_num:4*(cls_num+1)]\n",
    "                tx /= C.classifier_regr_std[0]\n",
    "                ty /= C.classifier_regr_std[1]\n",
    "                tw /= C.classifier_regr_std[2]\n",
    "                th /= C.classifier_regr_std[3]\n",
    "                x, y, w, h = apply_regr(x, y, w, h, tx, ty, tw, th)\n",
    "            except:\n",
    "                pass\n",
    "            bboxes[cls_name].append([C.rpn_stride*x, C.rpn_stride*y, C.rpn_stride*(x+w), C.rpn_stride*(y+h)])\n",
    "            probs[cls_name].append(np.max(P_cls[0, ii, :]))\n",
    "\n",
    "    all_dets = []\n",
    "\n",
    "    for key in bboxes:\n",
    "        bbox = np.array(bboxes[key])\n",
    "\n",
    "        new_boxes, new_probs = non_max_suppression_fast(bbox, np.array(probs[key]), overlap_thresh=0.2)\n",
    "        for jk in range(new_boxes.shape[0]):\n",
    "            (x1, y1, x2, y2) = new_boxes[jk,:]\n",
    "\n",
    "            # Calculate real coordinates on original image\n",
    "            (real_x1, real_y1, real_x2, real_y2) = get_real_coordinates(ratio, x1, y1, x2, y2)\n",
    "\n",
    "            cv2.rectangle(img,(real_x1, real_y1), (real_x2, real_y2), (int(class_to_color[key][0]), int(class_to_color[key][1]), int(class_to_color[key][2])),4)\n",
    "\n",
    "            textLabel = '{}: {}'.format(key,int(100*new_probs[jk]))\n",
    "            all_dets.append((key,100*new_probs[jk]))\n",
    "\n",
    "            (retval,baseLine) = cv2.getTextSize(textLabel,cv2.FONT_HERSHEY_COMPLEX,1,1)\n",
    "            textOrg = (real_x1, real_y1-0)\n",
    "\n",
    "            cv2.rectangle(img, (textOrg[0] - 5, textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (0, 0, 0), 1)\n",
    "            cv2.rectangle(img, (textOrg[0] - 5,textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (255, 255, 255), -1)\n",
    "            cv2.putText(img, textLabel, textOrg, cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 0), 1)\n",
    "\n",
    "    print('Elapsed time = {}'.format(time.time() - st))\n",
    "    print(all_dets)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.grid()\n",
    "    plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mAPs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-ea9677118a3a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmAP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmAP\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmAP\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmAPs\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmAP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;34m'nan'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmean_average_prec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmAP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'After training %dk batches, the mean average precision is %0.3f'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_average_prec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# record_df.loc[len(record_df)-1, 'mAP'] = mean_average_prec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mAPs' is not defined"
     ]
    }
   ],
   "source": [
    "mAP = [mAP for mAP in mAPs if str(mAP)!='nan']\n",
    "mean_average_prec = round(np.mean(np.array(mAP)), 3)\n",
    "print('After training %dk batches, the mean average precision is %0.3f'%(len(record_df), mean_average_prec))\n",
    "\n",
    "# record_df.loc[len(record_df)-1, 'mAP'] = mean_average_prec\n",
    "# record_df.to_csv(C.record_path, index=0)\n",
    "# print('Save mAP to {}'.format(C.record_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
